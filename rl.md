# <center> Reinforcement Learning Knowledge Memo </center>
[toc]


## 0. 写在前面
![](figures/rl/0.png)

## 1. 强化学习定义
一个强化学习任务可以用四元组$E = <X,A,P,R>$表示，其中：
$X$是状态空间，每个状态$x \in X$是机器感知到的环境的描述
$A$是动作空间，每个动作$a \in A$是机器在环境中所能采取的动作。
$ P : X \times A \times X \mapsto \Bbb{R}$指定了状态转移概率;
$ R : X \times A \times X \mapsto \Bbb{R}$指定了奖赏
当某个动作$a$作用在状态$x$上时，潜在的转移概率$P$将使环境从当前状态按某概率转移到另一个状态，在状态发生移转的同时，环境会根据潜在的奖赏(reward)函数$R$反馈给机器一个奖励。

![](figures/rl/1.png)

## 2. K-摇臂赌博机

## 3. 有模型学习
考虑多步强化学习任务3 暂且先假定任务对应的马尔可夫决策过程四元组$E=<X,A,P,R>$ 均为己知，这样的情形称为“模型已经”，即机器对环境进行了建模，能在机器内部模拟出与环境相同或近似的状况。在己知模型的环境中学习称为"有模型学习" (model-based learning)。此时，对于任意状态和动作$a$，在$x$状态下执行动作$a$转移到状态$x'$的概率$P^{a}_{x \mapsto x'}$ 是己知的，该转移所带来的奖赏$R^{a}_{x \mapsto x'}$也是已知的。为便于讨论,不妨假设状态空间$X$和动作空间$A$均为有限.


## 4. 免模型学习

在实际的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚至很难知道环境中一共有多少状态，若学习算法不依赖于环境建模，则称为“免模型学习”(model-free learning),这比有模型学习要困难得多。

### 4.1 蒙特卡罗强化学习

在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由于模型未知而导致无法做全概率展开。此时只能通过在环境中执行选择的动作，来观察转移的状态和得到的奖赏.受$K$摇臂赌博机的启发，一种直接的策略评估替代方法是多次"采样"，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习。由于采样必须为有限次数，因此该方法更适合于使用$T$步累积奖赏的强化学习任务。

&emsp;&emsp;另一方面， 策略迭代算法估计的是状态值函数$V$， 而最终的策略是通过状态-动作值函数$Q$来获得。当模型己知时，从$V$到$Q$有很简单的转换方法，而当模型未知时，这也会出现困难。于是，我们将估计对象从$V$转变为$Q$，即估计每一对"状态-动作"的值函数。

